{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253 0.5904970115\n",
      "717 0.600456461153\n",
      "761 0.580378332217\n",
      "998 0.737453861384\n",
      "439 0.668473367389\n",
      "443 0.620994280013\n",
      "283 0.59603436232\n",
      "565 0.622919265874\n",
      "586 0.657950585767\n",
      "374 0.610265376598\n",
      "0.628542290421\n"
     ]
    }
   ],
   "source": [
    "# This code is used to tune the parameters of XGBoost by successive grid search in the parameter space, as well as finding\n",
    "# the optimal number of trees by cross-validation.\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from calculate_auc import calculate_auc\n",
    "from utils import stratify\n",
    "\n",
    "n_boot = 10\n",
    "split = 0.9\n",
    "\n",
    "ft_dir = 'features_long/'\n",
    "\n",
    "# list feature files\n",
    "files = os.listdir(ft_dir)\n",
    "files = files[:40]\n",
    "\n",
    "# reading top locations\n",
    "with open('top_locations.dat', 'rb') as f:\n",
    "    location_top = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "feature_all = []\n",
    "target_all = []\n",
    "\n",
    "for filename in files:\n",
    "    with open(ft_dir+filename, 'rb') as f:  \n",
    "        feature, target = pickle.load(f)\n",
    "\n",
    "        # only keeping locations in location_top and encoding them\n",
    "        ind = np.array([], int)\n",
    "        for (i,loc) in enumerate(target['location']):\n",
    "            if loc in location_top:\n",
    "                ind = np.append(ind, i)\n",
    "                target.loc[i,'elocation'] = np.where(location_top==loc)[0]\n",
    "        feature = feature.loc[ind,:]\n",
    "        target = target.loc[ind]\n",
    "        \n",
    "        feature = feature.reset_index(drop=True)\n",
    "        target = target.reset_index(drop=True)\n",
    "        \n",
    "        # change encoded column data type to int\n",
    "        target['elocation'] = target['elocation'].astype(int)\n",
    "        \n",
    "        feature_all.append(feature)\n",
    "        target_all.append(target)\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "\n",
    "# for max_depth in np.arange(3,10,2):\n",
    "#     for min_child_weight in np.arange(1,6,2):\n",
    "# for max_depth in np.arange(2,5,1):\n",
    "#     for min_child_weight in np.arange(4,7,1):\n",
    "# for gamma in [k/10.0 for k in range(0,5)]:\n",
    "# for gamma in [k/10.0 for k in range(4,7)]:\n",
    "# for subsample in [k/10.0 for k in range(6,10)]:\n",
    "#     for colsample_bytree in [kk/10.0 for kk in range(6,10)]:\n",
    "# for subsample in np.arange(.75,9,.05):\n",
    "#     for colsample_bytree in np.arange(.85,1,.05):\n",
    "# for reg_alpha in [1e-5, 1e-2, 0.1, 1, 100]:\n",
    "# for reg_alpha in [1e-7, 1e-6, 1e-5]:\n",
    "\n",
    "confs = []\n",
    "aucs = []\n",
    "aucs_mean = []\n",
    "labels = []\n",
    "inds = np.arange(0,len(feature_all),1)\n",
    "inds_split = int(np.floor(split*len(feature_all)))\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "for i in range(n_boot):\n",
    "\n",
    "    # training set\n",
    "    np.random.shuffle(inds)\n",
    "    ind_train = inds[:inds_split]\n",
    "    ind_test = inds[inds_split:]\n",
    "\n",
    "    x_train = pd.concat([feature_all[j] for j in ind_train], axis=0)\n",
    "    y_train = pd.concat([target_all[j]['elocation'] for j in ind_train], axis=0)\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "    # stratification\n",
    "    (x_train, y_train) = stratify(x_train, y_train)\n",
    "\n",
    "    # test set\n",
    "    x_test = pd.concat([feature_all[j] for j in ind_test], axis=0)\n",
    "    y_test = pd.concat([target_all[j]['elocation'] for j in ind_test], axis=0)\n",
    "    x_test = x_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "    # remove foursquare features (sensor)\n",
    "#     x_train = x_train.drop(['fsq 0','fsq 1','fsq 2','fsq 3','fsq 4','fsq 5','fsq 6','fsq 7','fsq 8','fsq distance'],axis=1)\n",
    "#     x_test = x_test.drop(['fsq 0','fsq 1','fsq 2','fsq 3','fsq 4','fsq 5','fsq 6','fsq 7','fsq 8','fsq distance'],axis=1)\n",
    "#     x_train = x_train.reset_index(drop=True)\n",
    "#     x_test = x_test.reset_index(drop=True)\n",
    "\n",
    "    # model (sensor)\n",
    "#     gbm = xgb.XGBClassifier(max_depth=6, n_estimators=50, learning_rate=0.05, nthread=12, subsample=0.25, \\\n",
    "#                         colsample_bytree=0.5, max_delta_step=0, gamma=3, objective='mlogloss', reg_alpha=0.5, \\\n",
    "#                         missing=np.nan)\n",
    "\n",
    "    # model (sensor + foursquare)\n",
    "    gbm = xgb.XGBClassifier(max_depth=4, n_estimators=1000, learning_rate=0.01, nthread=12, subsample=0.8, \\\n",
    "                        colsample_bytree=0.9, max_delta_step=0, gamma=0.4, objective='mlogloss', \\\n",
    "                        reg_alpha=0, reg_lambda=1, \\\n",
    "                        missing=np.nan, min_child_weight=4, scale_pos_weight=1, seed=27)\n",
    "\n",
    "    # for finding the best n_estimators\n",
    "    gbm.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric='mlogloss', verbose=False,\\\n",
    "           early_stopping_rounds=50)\n",
    "\n",
    "    # after finding the best n_estimators\n",
    "#     gbm.fit(x_train, y_train)\n",
    "\n",
    "    # training performance\n",
    "    y_pred = gbm.predict(x_train)\n",
    "    conf_train, roc_auc_train = calculate_auc(y_pred, y_train, location_top.size)\n",
    "\n",
    "    # test\n",
    "    y_pred = gbm.predict(x_test)\n",
    "    conf, roc_auc = calculate_auc(y_pred, y_test, location_top.size)\n",
    "\n",
    "    labels.append(np.unique(y_test))\n",
    "    confs.append(conf)\n",
    "    aucs.append(roc_auc)\n",
    "    aucs_mean.append(np.nanmean(roc_auc))\n",
    "    print(gbm.best_iteration, np.nanmean(roc_auc))\n",
    "#         print(np.nanmean(roc_auc))\n",
    "\n",
    "#     print(np.unique(y_test))\n",
    "#     print(roc_auc_train, np.nanmean(roc_auc_train))\n",
    "#     print(roc_auc, np.nanmean(roc_auc))\n",
    "\n",
    "print(np.nanmean(np.array(aucs_mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(2,5,1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
