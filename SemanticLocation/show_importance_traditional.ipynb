{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This code uses the traditional measure of features importance, i.e. the increase in validation error by removing each feature\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from calculate_confusion_matrix import calculate_confusion_matrix\n",
    "\n",
    "ft_dir = 'features_long/'\n",
    "\n",
    "# list feature files\n",
    "files = os.listdir(ft_dir)\n",
    "\n",
    "with open('top_locations.dat') as f:\n",
    "    state_top10 = pickle.load(f)\n",
    "f.close()\n",
    "# for (i,s) in enumerate(state_top10):\n",
    "#     state_top10[i] = state_top10[i].replace('\"','')\n",
    "#     state_top10[i] = state_top10[i].replace('[','')\n",
    "#     state_top10[i] = state_top10[i].replace(']','')\n",
    "\n",
    "feature_all = []\n",
    "state_all = []\n",
    "state_fsq_all = []\n",
    "for filename in files:\n",
    "    with open(ft_dir+filename) as f:  \n",
    "        feature, state = pickle.load(f)\n",
    "        \n",
    "        # only keeping top 10 states\n",
    "        ind = np.array([], int)\n",
    "        for (i,st) in enumerate(state['location']):\n",
    "            if st in state_top10:\n",
    "                ind = np.append(ind, i)\n",
    "        feature = feature.loc[ind,:]\n",
    "        state = state.loc[ind,'location']\n",
    "        \n",
    "        feature = feature.reset_index(drop = True)\n",
    "        state = state.reset_index(drop = True)\n",
    "        \n",
    "        feature_all.append(feature)\n",
    "        state_all.append(state)\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "n_fold = 10\n",
    "n = len(feature_all)\n",
    "fold_size= n/float(n_fold)\n",
    "    \n",
    "\n",
    "# reference AUC\n",
    "auc_ref = np.zeros([n_fold])\n",
    "for k in range(n_fold):\n",
    "\n",
    "    ind_test = np.arange(np.ceil(k*fold_size), min(np.ceil((k+1)*fold_size),n)).astype(int)\n",
    "    ind_train = np.delete(np.arange(n), ind_test)\n",
    "\n",
    "    x_train = pd.concat([feature_all[j] for j in ind_train], axis=0)\n",
    "    y_train = pd.concat([state_all[j] for j in ind_train], axis=0)\n",
    "    x_test = pd.concat([feature_all[j] for j in ind_test], axis=0)\n",
    "    y_test = pd.concat([state_all[j] for j in ind_test], axis=0)\n",
    "\n",
    "    x_train = x_train.reset_index(drop = True)\n",
    "    y_train = y_train.reset_index(drop = True)\n",
    "    x_test = x_test.reset_index(drop = True)\n",
    "    y_test = y_test.reset_index(drop = True)\n",
    "\n",
    "    gbm = xgb.XGBClassifier(max_depth=6, n_estimators=75, learning_rate=0.05, nthread=12, subsample=0.25, \\\n",
    "                        colsample_bytree=0.2, max_delta_step=0, gamma=3, objective='mlogloss', reg_alpha=0.5, \\\n",
    "                        missing=np.nan, seed=0)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    y_pred = gbm.predict(x_test)\n",
    "    conf, roc_auc = calculate_confusion_matrix(y_pred, y_test)\n",
    "    auc_ref[k] = np.nanmean(roc_auc)\n",
    "\n",
    "# auc_ref = np.nanmean(auc_fold)\n",
    "    \n",
    "# removing features one at each time\n",
    "delta_auc_mean = np.zeros([feature.shape[1]])\n",
    "delta_auc_lo = np.zeros([feature.shape[1]])\n",
    "delta_auc_hi = np.zeros([feature.shape[1]])\n",
    "for (i,ft) in enumerate(feature.columns):\n",
    "    print i,\n",
    "    auc_fold = np.zeros([n_fold])\n",
    "    for k in range(n_fold):\n",
    "\n",
    "        ind_test = np.arange(np.ceil(k*fold_size), min(np.ceil((k+1)*fold_size),n)).astype(int)\n",
    "        ind_train = np.delete(np.arange(n), ind_test)\n",
    "\n",
    "        x_train = pd.concat([feature_all[j] for j in ind_train], axis=0)\n",
    "        y_train = pd.concat([state_all[j] for j in ind_train], axis=0)\n",
    "        x_test = pd.concat([feature_all[j] for j in ind_test], axis=0)\n",
    "        y_test = pd.concat([state_all[j] for j in ind_test], axis=0)\n",
    "\n",
    "        x_train = x_train.reset_index(drop = True)\n",
    "        y_train = y_train.reset_index(drop = True)\n",
    "        x_test = x_test.reset_index(drop = True)\n",
    "        y_test = y_test.reset_index(drop = True)\n",
    "        \n",
    "        x_train = x_train.drop([ft], axis=1)\n",
    "        x_test = x_test.drop([ft], axis=1)\n",
    "        \n",
    "        gbm = xgb.XGBClassifier(max_depth=6, n_estimators=75, learning_rate=0.05, nthread=12, subsample=0.25, \\\n",
    "                            colsample_bytree=0.2, max_delta_step=0, gamma=3, objective='mlogloss', reg_alpha=0.5, \\\n",
    "                            missing=np.nan, seed=0)\n",
    "        gbm.fit(x_train, y_train)\n",
    "        y_pred = gbm.predict(x_test)\n",
    "        conf, roc_auc = calculate_confusion_matrix(y_pred, y_test)\n",
    "        auc_fold[k] = np.nanmean(roc_auc)\n",
    "        \n",
    "    delta_auc_mean[i] = np.nanmean(auc_ref-auc_fold)\n",
    "    delta_auc_lo[i] = np.nanmin(auc_ref-auc_fold)\n",
    "    delta_auc_hi[i] = np.nanmax(auc_ref-auc_fold)\n",
    "    print delta_auc_mean[i], delta_auc_lo[i], delta_auc_hi[i]\n",
    "    \n",
    "with open('feature_importance.dat','w') as f:\n",
    "    pickle.dump([delta_auc_mean,delta_auc_lo,delta_auc_hi],f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "[Errno 2] No such file or directory: 'feature_importance.dat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d054f4d5829e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'feature_importance.dat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdelta_auc_mean\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_auc_lo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdelta_auc_hi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'feature_importance.dat'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with open('feature_importance.dat') as f:\n",
    "    delta_auc_mean,delta_auc_lo,delta_auc_hi = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "dic = {'lgt mean':'Light Intensity Mean', 'lgt std':'Light Intensity Variance', 'lgt off':'Darkness Duration', 'lgt zcrossing':'Light Intensity Change',\\\n",
    "       'lgt skew':'Light Intensity Skewness', 'lgt kurt':'Light Intensity Kurtosis', 'aud mean':'Sound Amplitude Mean', \\\n",
    "       'aud std':'Sound Amplitude Variance', 'aud skew':'Sound Amplitude Skewness', 'aud kurt':'Sound Amplitude Kurtosis',\\\n",
    "       'aud frq mean':'Sound Frequency Mean', 'aud frq std':'Sound Frequency Variance', 'aud frq skew':'Sound Frequency Skewness',\\\n",
    "       'aud frq kurt':'Sound Frequency Kurtosis', 'scr frq':'Screen On/Off Frequency', 'scr dur mean':'Screen On Duration Mean', \\\n",
    "       'scr dur std':'Screen On Duration Variance', 'still':'Activity \"Still\" Time', 'tilting':'Activity \"Tilting\" Time', 'walking':'Activity \"Walking\" Time',\\\n",
    "       'unknown act':'Activity \"Unknown\" Time', 'still-walking':'\"Still\"-\"Walking\" Transition', 'still-tilting':'\"Still\"-\"Tilting\" Transition',\\\n",
    "       'still-unknown':'\"Still\"-\"Unknown\" Transition', 'walking-unknown':'\"Walking\"-\"Unknown\" Transition', 'call in':'Incoming Calls', 'call out':\\\n",
    "       'Outgoing Calls', 'sms in':'Incoming SMS', 'sms out':'Outgoing SMS', 'call missed':'Missed Calls', 'n wifi':\\\n",
    "       'Wifi Networks', 'temperature':'Outside Temperature', 'dew point':'Outside Windchill', 'weather':'Weather Condition', \\\n",
    "       'lat mean':'Latitude', 'lng mean':'Longitude', 'loc var':'Location Variance', 'duration':'Visit Timespan',\\\n",
    "       'midtime':'Visit Unix Timestamp', 'midhour':'Visit Time of Day', 'dow start':'Arrive Day of Week', 'dow end':\\\n",
    "       'Leave Day of Week', 'fsq 0':'Foursquare Nightlife Spot', 'fsq 1':'Foursquare Outdoors & Recreation', 'fsq 2':'Foursquare Arts & Entertainment'\\\n",
    "       , 'fsq 3':'Foursquare Professional or Medical Office', 'fsq 4':'Foursquare Food', 'fsq 5':'Foursquare Home', \\\n",
    "       'fsq 6':'Foursquare Shop or Store', 'fsq 7':'Foursquare Travel or Transport', 'fsq 8':'Foursquare Unknown', 'fsq distance':\\\n",
    "       'Foursquare Distance to Closest Location', 'LT frequency':'Visit Frequency', 'LT interval mean':'Mean Time Between Visits', 'n gps':'Visit Duration'}\n",
    "\n",
    "# extracting means and CIs\n",
    "feature_label = feature_all[0].columns\n",
    "\n",
    "# fscore_ci = np.array(fscore.std(axis=0)/np.sqrt(n_bootstrap))\n",
    "ind_sort = np.array(np.argsort(delta_auc_mean))\n",
    "delta_auc_mean_sorted = delta_auc_mean[ind_sort]\n",
    "# ci_sorted = fscore_ci[ind_sort]\n",
    "feature_label_sorted = feature_label[ind_sort]\n",
    "feature_label_short = []\n",
    "for i in range(feature_label_sorted.size):\n",
    "    feature_label_short.append(dic[feature_label_sorted[i]])\n",
    "    \n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(5,12))\n",
    "plt.barh(np.arange(delta_auc_mean_sorted.size), delta_auc_mean_sorted, height=.7, color=(.4,.4,.8), align='center', ecolor=(0,0,0))\n",
    "plt.yticks(np.arange(len(feature_label_short)), feature_label_short, fontsize=10, color=(0,0,0))\n",
    "# plt.box(on=False)\n",
    "plt.xlabel('Decrease in AUC',fontsize=14)\n",
    "plt.axes().yaxis.tick_right()\n",
    "plt.ylim([-1, len(feature_label_short)])\n",
    "plt.grid()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
