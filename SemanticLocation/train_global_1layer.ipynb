{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from calculate_auc import calculate_auc\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from utils import stratify\n",
    "\n",
    "save_results = True\n",
    "do_stratify = False\n",
    "\n",
    "n_boot = 100\n",
    "split = 0.7\n",
    "np.random.seed(seed=0)\n",
    "\n",
    "ft_dir = 'features_long/'\n",
    "\n",
    "# list feature files\n",
    "files = os.listdir(ft_dir)\n",
    "# files.remove('1515656.dat')\n",
    "# print('subject 1515656 removed due to spoofed gps.')\n",
    "\n",
    "# reading top locations\n",
    "with open('top_locations.dat', 'rb') as f:\n",
    "    location_top = pickle.load(f)\n",
    "f.close()\n",
    "\n",
    "feature_all = []\n",
    "target_all = []\n",
    "\n",
    "for filename in files:\n",
    "    with open(ft_dir+filename, 'rb') as f:  \n",
    "        feature, target = pickle.load(f)\n",
    "\n",
    "        # only keeping locations in location_top and encoding them\n",
    "        ind = np.array([], int)\n",
    "        for (i,loc) in enumerate(target['location']):\n",
    "            if loc in location_top:\n",
    "                ind = np.append(ind, i)\n",
    "                target.loc[i,'elocation'] = np.where(location_top==loc)[0]\n",
    "        feature = feature.loc[ind,:]\n",
    "        target = target.loc[ind]\n",
    "        feature = feature.reset_index(drop=True)\n",
    "        target = target.reset_index(drop=True)\n",
    "        \n",
    "        # change encoded column data type to int\n",
    "        target['elocation'] = target['elocation'].astype(int)\n",
    "        \n",
    "        feature_all.append(feature)\n",
    "        target_all.append(target)\n",
    "        \n",
    "    f.close()\n",
    "\n",
    "confs = []\n",
    "aucs = []\n",
    "labels = []\n",
    "inds = np.arange(0,len(feature_all),1)\n",
    "inds_split = int(np.floor(split*len(feature_all)))\n",
    "\n",
    "for i in range(n_boot):\n",
    "    \n",
    "    print('------------------')\n",
    "    print(i)\n",
    "    \n",
    "    # training set\n",
    "    np.random.shuffle(inds)\n",
    "    ind_train = inds[:inds_split]\n",
    "    ind_test = inds[inds_split:]\n",
    "    \n",
    "    x_train = pd.concat([feature_all[j] for j in ind_train], axis=0)\n",
    "    y_train = pd.concat([target_all[j]['elocation'] for j in ind_train], axis=0)\n",
    "    x_train = x_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    \n",
    "    # stratification\n",
    "    (x_train, y_train) = stratify(x_train, y_train)\n",
    "    \n",
    "    # test set\n",
    "    x_test = pd.concat([feature_all[j] for j in ind_test], axis=0)\n",
    "    y_test = pd.concat([target_all[j]['elocation'] for j in ind_test], axis=0)\n",
    "    x_test = x_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    \n",
    "    # remove foursquare features\n",
    "#     x_train = x_train.drop(['fsq 0','fsq 1','fsq 2','fsq 3','fsq 4','fsq 5','fsq 6','fsq 7','fsq 8','fsq distance'],axis=1)\n",
    "#     x_test = x_test.drop(['fsq 0','fsq 1','fsq 2','fsq 3','fsq 4','fsq 5','fsq 6','fsq 7','fsq 8','fsq distance'],axis=1)\n",
    "#     x_train = x_train.reset_index(drop=True)\n",
    "#     x_test = x_test.reset_index(drop=True)\n",
    "    \n",
    "    # model (sensor)\n",
    "#     gbm = xgb.XGBClassifier(max_depth=6, n_estimators=50, learning_rate=0.05, nthread=12, subsample=0.25, \\\n",
    "#                         colsample_bytree=0.5, max_delta_step=0, gamma=3, objective='mlogloss', reg_alpha=0.5, \\\n",
    "#                         missing=np.nan)\n",
    "#     gbm = xgb.XGBClassifier(max_depth=4, n_estimators=200, learning_rate=0.025, nthread=12, subsample=0.2, \\\n",
    "#                         colsample_bytree=0.5, max_delta_step=0, gamma=0.4, objective='mlogloss', reg_alpha=0, \\\n",
    "#                         reg_lambda=1, missing=np.nan, min_child_weight=4)\n",
    "    \n",
    "    # model (sensor + foursquare)\n",
    "#     gbm = xgb.XGBClassifier(max_depth=6, n_estimators=75, learning_rate=0.05, nthread=12, subsample=0.25, \\\n",
    "#                         colsample_bytree=0.2, max_delta_step=0, gamma=3, objective='mlogloss', reg_alpha=0.5, \\\n",
    "#                         missing=np.nan)\n",
    "    gbm = xgb.XGBClassifier(max_depth=4, n_estimators=300, learning_rate=0.025, nthread=12, subsample=0.25, \\\n",
    "                        colsample_bytree=0.2, max_delta_step=0, gamma=0.4, objective='mlogloss', reg_alpha=0, \\\n",
    "                        reg_lambda=1, missing=np.nan, min_child_weight=4)\n",
    "    \n",
    "    # fitting model\n",
    "#     gbm.fit(x_train, y_train, eval_set=[(x_test, y_test)], eval_metric='mlogloss', verbose=False, early_stopping_rounds=50)\n",
    "#     print('best n_estimators: ',gbm.best_iteration)\n",
    "    gbm.fit(x_train, y_train)\n",
    "    \n",
    "    # training performance\n",
    "    y_pred = gbm.predict(x_train)\n",
    "    conf_train, roc_auc_train = calculate_auc(y_pred, y_train, location_top.size)\n",
    "\n",
    "    # test\n",
    "    y_pred = gbm.predict(x_test)\n",
    "    conf, roc_auc = calculate_auc(y_pred, y_test, location_top.size)\n",
    "    \n",
    "#     labels.append(np.unique(y_test))\n",
    "    confs.append(conf)\n",
    "    aucs.append(roc_auc)\n",
    "\n",
    "#     print(np.unique(y_test))\n",
    "    print(location_top)\n",
    "    print(roc_auc_train, np.nanmean(roc_auc_train))\n",
    "    print(roc_auc, np.nanmean(roc_auc))\n",
    "\n",
    "if save_results:\n",
    "    with open('auc_all.dat','wb') as f:\n",
    "        pickle.dump([aucs, confs, labels], f)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(y_train);\n",
    "plt.hist(y_test);\n",
    "plt.hist(y_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "print(roc_auc_score(np.array([0,0,0,0,0,0,0,0]),np.array([0,0,1,0,0,0,0,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
